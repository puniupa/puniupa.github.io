<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.552">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="ぷに">

<title>ぷにうぱ - これからはじめるベイズ機械学習</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../">
<link href="../../../assets/ぷにうぱ.jpg" rel="icon" type="image/jpeg">
<script src="../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script src="../../../site_libs/quarto-contrib/videojs/video.min.js"></script>
<link href="../../../site_libs/quarto-contrib/videojs/video-js.css" rel="stylesheet">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "?",
    "H"
  ],
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="">
<link href="https://fonts.googleapis.com/css2?family=Zen+Kurenaido&amp;display=swap" rel="stylesheet">

<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="">
<link href="https://fonts.googleapis.com/css2?family=BIZ+UDPGothic&amp;display=swap" rel="stylesheet">

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../../assets/styles.css">
<meta property="og:title" content="ぷにうぱ - これからはじめるベイズ機械学習">
<meta property="og:description" content="所信表明を兼ねて">
<meta property="og:image" content="https://puniupa.github.io/posts/2024/AI/assets/ぷにうぱ.jpg">
<meta property="og:site_name" content="ぷにうぱ">
<meta name="twitter:title" content="ぷにうぱ - これからはじめるベイズ機械学習">
<meta name="twitter:description" content="所信表明を兼ねて">
<meta name="twitter:image" content="https://puniupa.github.io/posts/2024/AI/assets/ぷにうぱ.jpg">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../../index.html">
    <span class="navbar-title">ぷにうぱ</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../../static/AllCategories.html"> 
<span class="menu-text">カテゴリ一覧</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../static/AboutUs.html"> 
<span class="menu-text">自己紹介</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
          <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">目次</h2>
   
  <ul>
  <li><a href="#ベイズ機械学習のすすめ" id="toc-ベイズ機械学習のすすめ" class="nav-link active" data-scroll-target="#ベイズ機械学習のすすめ"><span class="header-section-number">1</span> ベイズ機械学習のすすめ</a>
  <ul class="collapse">
  <li><a href="#ベイズとは何か" id="toc-ベイズとは何か" class="nav-link" data-scroll-target="#ベイズとは何か"><span class="header-section-number">1.1</span> ベイズとは何か？</a></li>
  <li><a href="#ベイズと頻度論との違い" id="toc-ベイズと頻度論との違い" class="nav-link" data-scroll-target="#ベイズと頻度論との違い"><span class="header-section-number">1.2</span> ベイズと頻度論との違い</a></li>
  <li><a href="#つの世界樹" id="toc-つの世界樹" class="nav-link" data-scroll-target="#つの世界樹"><span class="header-section-number">1.3</span> ２つの世界樹</a></li>
  </ul></li>
  <li><a href="#sec-uncertainty-quantification" id="toc-sec-uncertainty-quantification" class="nav-link" data-scroll-target="#sec-uncertainty-quantification"><span class="header-section-number">2</span> ベイズは不確実性を定量化する</a>
  <ul class="collapse">
  <li><a href="#sec-need-for-uncertainty-quantification" id="toc-sec-need-for-uncertainty-quantification" class="nav-link" data-scroll-target="#sec-need-for-uncertainty-quantification"><span class="header-section-number">2.1</span> 不確実性の定量化の必要性</a></li>
  <li><a href="#信頼のおける-ai-システム" id="toc-信頼のおける-ai-システム" class="nav-link" data-scroll-target="#信頼のおける-ai-システム"><span class="header-section-number">2.2</span> 信頼のおける AI システム</a></li>
  <li><a href="#sec-Bayes-for-uncertainty-quatification" id="toc-sec-Bayes-for-uncertainty-quatification" class="nav-link" data-scroll-target="#sec-Bayes-for-uncertainty-quatification"><span class="header-section-number">2.3</span> 不確実性を扱うには Bayes が必要である</a></li>
  <li><a href="#ベイズ深層学習という夢" id="toc-ベイズ深層学習という夢" class="nav-link" data-scroll-target="#ベイズ深層学習という夢"><span class="header-section-number">2.4</span> ベイズ深層学習という夢</a></li>
  <li><a href="#分野全体の動向" id="toc-分野全体の動向" class="nav-link" data-scroll-target="#分野全体の動向"><span class="header-section-number">2.5</span> 分野全体の動向</a></li>
  </ul></li>
  <li><a href="#sec-distributional-representation" id="toc-sec-distributional-representation" class="nav-link" data-scroll-target="#sec-distributional-representation"><span class="header-section-number">3</span> ベイズは分布という共通言語を与える</a>
  <ul class="collapse">
  <li><a href="#継続学習という発想" id="toc-継続学習という発想" class="nav-link" data-scroll-target="#継続学習という発想"><span class="header-section-number">3.1</span> 継続学習という発想</a></li>
  <li><a href="#例強化学習への分布によるアプローチ" id="toc-例強化学習への分布によるアプローチ" class="nav-link" data-scroll-target="#例強化学習への分布によるアプローチ"><span class="header-section-number">3.2</span> 例：強化学習への分布によるアプローチ</a></li>
  </ul></li>
  <li><a href="#sec-inductive-bias" id="toc-sec-inductive-bias" class="nav-link" data-scroll-target="#sec-inductive-bias"><span class="header-section-number">4</span> ベイズは理解を促進する</a>
  <ul class="collapse">
  <li><a href="#なぜベイズ法の発展が遅れたか" id="toc-なぜベイズ法の発展が遅れたか" class="nav-link" data-scroll-target="#なぜベイズ法の発展が遅れたか"><span class="header-section-number">4.1</span> なぜベイズ法の発展が遅れたか？</a></li>
  <li><a href="#帰納バイアスの明確化の必要性" id="toc-帰納バイアスの明確化の必要性" class="nav-link" data-scroll-target="#帰納バイアスの明確化の必要性"><span class="header-section-number">4.2</span> 帰納バイアスの明確化の必要性</a></li>
  <li><a href="#数学者の哲学" id="toc-数学者の哲学" class="nav-link" data-scroll-target="#数学者の哲学"><span class="header-section-number">4.3</span> 数学者の哲学</a></li>
  <li><a href="#sec-Bayesian-rule" id="toc-sec-Bayesian-rule" class="nav-link" data-scroll-target="#sec-Bayesian-rule"><span class="header-section-number">4.4</span> ベイズ推論とみる美点</a></li>
  </ul></li>
  <li><a href="#bayes-機械学習の例" id="toc-bayes-機械学習の例" class="nav-link" data-scroll-target="#bayes-機械学習の例"><span class="header-section-number">5</span> Bayes 機械学習の例</a>
  <ul class="collapse">
  <li><a href="#bayes-深層学習" id="toc-bayes-深層学習" class="nav-link" data-scroll-target="#bayes-深層学習"><span class="header-section-number">5.1</span> Bayes 深層学習</a></li>
  <li><a href="#sec-graphical-model" id="toc-sec-graphical-model" class="nav-link" data-scroll-target="#sec-graphical-model"><span class="header-section-number">5.2</span> 確率的グラフィカルモデル</a></li>
  <li><a href="#確率的プログラミング" id="toc-確率的プログラミング" class="nav-link" data-scroll-target="#確率的プログラミング"><span class="header-section-number">5.3</span> 確率的プログラミング</a></li>
  <li><a href="#bayes-最適化" id="toc-bayes-最適化" class="nav-link" data-scroll-target="#bayes-最適化"><span class="header-section-number">5.4</span> Bayes 最適化</a></li>
  <li><a href="#確率的データ圧縮" id="toc-確率的データ圧縮" class="nav-link" data-scroll-target="#確率的データ圧縮"><span class="header-section-number">5.5</span> 確率的データ圧縮</a></li>
  <li><a href="#モデルの自動発見" id="toc-モデルの自動発見" class="nav-link" data-scroll-target="#モデルの自動発見"><span class="header-section-number">5.6</span> モデルの自動発見</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">これからはじめるベイズ機械学習</h1>
<p class="subtitle lead">所信表明を兼ねて</p>
  <div class="quarto-categories">
    <div class="quarto-category">ベイズ統計</div>
    <div class="quarto-category">AI</div>
  </div>
  </div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">著者</div>
    <div class="quarto-title-meta-contents">
             <p>ぷに </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">日付</div>
    <div class="quarto-title-meta-contents">
      <p class="date">3/20/2024</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<p>現在，産業界における “AI” というと専ら，いくつかの限られた巨大 IT 企業が，巨大ニューラルネットワークを最尤推定で学習させ，これを基盤モデルとして公開し，我々一般庶民はそれを有効活用して下流タスクを安価に解くことだけ考えるという営みを指す．</p>
<p>その産業や生活への破壊的な影響を憂慮しながらも，雨乞いをする日々である．</p>
<div id="fig-Altman" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-Altman-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video"><video id="video_shortcode_videojs_video1" class="video-js vjs-default-skin vjs-fluid" controls="" preload="auto" data-setup="{}" title=""><source src="Images/SamAltman.mp4"></video></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-Altman-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
図&nbsp;1: 3月19日時点，GPT-5 にも Sora にもアクセス権を持たない我々 v.s. <a href="https://youtu.be/jvqFAi7vkBc?si=hwF_LJAs7XE3bNTR&amp;t=2695">Lex Fridman Podcast</a> に出演した Sam Altman
</figcaption>
</figure>
</div>
<p>AI はそんなものではない．AI はこれにかぎるものではない．</p>
<p>AI が真に我々の友となり，我々の日常をほんとうに豊かにするは，AI の進歩だけが必要なのではなく，<strong>人間との協業が得意になる必要がある</strong>．</p>
<p>そのための第一歩はすでに明らかである．<strong>不確実性の定量化</strong> である．</p>
<p>つまり，「その AI には何が出来て何が出来ないか」「AI の出力がいつ信頼にたるもので，いつ人間の介入が必要であるのか」がわかりやすい形で伝わるコミュニケーション様式をそなえている必要があるのである．<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a></p>
<hr>
<p>筆者の知る限り，ここにある全てのナラティブは現時点では全く広く語られているものではなく，筆者も最初の１年の研究生活を通じて朧げながら見えて来たばかりのものである．</p>
<p><strong>不確実性の定量化は，機械学習モデルを民主化し，我々の民芸に取り込むための重要な一歩である</strong>（のではないだろうか？）．</p>
<p>本稿はこの発見を共有するために書いた．筆者の反芻不足から，冗長な部分も多いだろうが，少しでも，琴線に触れるものがあれば幸いである．<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a></p>
<div class="hidden">
<p>A Blog Entry on Bayesian Computation by an Applied Mathematician</p>
<p>$$</p>
<p>$$</p>
</div>
<section id="ベイズ機械学習のすすめ" class="level2" data-number="1">
<h2 data-number="1" class="anchored" data-anchor-id="ベイズ機械学習のすすめ"><span class="header-section-number">1</span> ベイズ機械学習のすすめ</h2>
<p>我々が AI をより信頼するためには，何が必要だろうか？</p>
<p>筆者の考えでは，信頼への第一歩は <strong>不確実性の定量化</strong> が出来るようになることのはずである．</p>
<p>そしてそのためには <strong>ベイズ機械学習</strong> (Bayesian Machine Learning) の発展による本質的解決が必要不可欠である．本稿はこの点を説明するために執筆されたものである．</p>
<p>筆者に言わせれば，ベイズ機械学習が，今後数年間で AI が経験すべき進展の方向である．この山を越えれば，今まででさえ思っても見なかった未来がひらけてくるだろう．</p>
<blockquote class="blockquote">
<p>Although considerable challenges remain, the coming decade promises substantial advances in artificial intelligence and machine learning based on the probabilistic framework. <span class="citation" data-cites="Ghahramani2015">(<a href="#ref-Ghahramani2015" role="doc-biblioref">Ghahramani, 2015, p. 452</a>)</span></p>
</blockquote>
<section id="ベイズとは何か" class="level3" data-number="1.1">
<h3 data-number="1.1" class="anchored" data-anchor-id="ベイズとは何か"><span class="header-section-number">1.1</span> ベイズとは何か？</h3>
<p>機械学習において，確率論的なモデリングに基づいたアプローチを <strong>ベイズ機械学習</strong> ともいう．典型的には，モデルの全変数上の結合分布をモデリングし，ベイズ規則によりパラメータのベイズ推定を行う，という手続きからなる．そのため，<strong>確率論的アプローチ</strong> や <strong>モデルベースアプローチ</strong> も同義語として用いられる．<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a></p>
<p>一方で，<strong>頻度論的</strong> という言葉は，よく非ベイズ的アプローチを示す接頭辞として用いられる．典型的には，損失関数を設定し，これを最小化するパラメータを探索することによって実行される．</p>
<p>この２つのアプローチは互いに対照的であり，統計学の始まりから基本的な二項対立の図式をなしてきた．</p>
<div class="table-responsive-sm">
<table class="table-striped table-hover table">
<caption>Contrast of the two main approachs to Machine Learning</caption>
<colgroup>
<col style="width: 20%">
<col style="width: 20%">
<col style="width: 10%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: center;"></th>
<th style="text-align: center;">Bayesian</th>
<th style="text-align: center;">Frequentist</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">Inference is<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a></td>
<td style="text-align: center;">Marginalization</td>
<td style="text-align: center;">Approximation</td>
</tr>
<tr class="even">
<td style="text-align: center;">Computational Idea<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a></td>
<td style="text-align: center;">Integration</td>
<td style="text-align: center;">Optimization</td>
</tr>
<tr class="odd">
<td style="text-align: center;">Objective</td>
<td style="text-align: center;">Uncertainty Quantification</td>
<td style="text-align: center;">Recovery of True Value</td>
</tr>
<tr class="even">
<td style="text-align: center;">Emphasis</td>
<td style="text-align: center;">Modelling</td>
<td style="text-align: center;">Inference</td>
</tr>
</tbody>
</table>
</div>
<p>しかし，機械学習の時代においては，互いの弱みを補間し合う形で発展していくと筆者は考える．特に，現状の<u><strong>推論偏重でモデリング軽視の風潮が，重要な実世界応用の多くを阻んでしまっている</strong></u>．機械学習の世界樹は実は２本あるのである．</p>
</section>
<section id="ベイズと頻度論との違い" class="level3" data-number="1.2">
<h3 data-number="1.2" class="anchored" data-anchor-id="ベイズと頻度論との違い"><span class="header-section-number">1.2</span> ベイズと頻度論との違い</h3>
<p>ベイズと頻度論では，確率の解釈も異なるかも知れないが，数学的枠組みとしてはベイズの方が一般的な枠組みであり，また手続き上は，モデリングを重視するか，推論を重視するかの違いでしかない．</p>
<p>実際，殆どの場合，頻度論的手法はある特定の事前分布を持ったベイズ手法とみなせ，逆も然りである．</p>
<p>データから推論を行うには，何らかの仮定が必ず必要であり，それを明示的にモデルに組み込むのがベイズで，推論アルゴリズムにより自動化する精神を持つのが頻度論的手法である．</p>
<p>その結果，優秀な推論アルゴリズムが日夜驚異的なスピードで提案され，今や機械学習手法は教師あり学習・教師なし学習・強化学習の全てで目覚ましい発展を見た．</p>
<p>しかし，ベイズと頻度論の２つの柱のバランスを欠いた発展はここまでである．今や，頻度論的な手法を採用した際に，自分たちがどのような仮定を置いたのか全く明瞭な知識を欠いてしまっている．一方で，現実のビッグで複雑なデータを扱うためには，もはや確率的なモデリングを避けては通れない．<a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a></p>
<p>極めて本質的で強大な敵に対面しつつあるのである．</p>
<p>だが，現状の病理は明らかであり，頻度論とベイズの手法の間に対応をつけ，足並みを揃えることで次の前進が約束されてる．この意味で，２つの世界樹が必要なのである．</p>
<p>さらに，ベイズ推論は帰納的推論の確率論的拡張と見れるため，エージェントの合理的な学習と意思決定の最良のモデル（の一つ）と信じられている．<a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a></p>
<p>したがって，<u><strong>ベイズ流解釈により手法を理解し，最適化流解釈により手法を実装する</strong></u>．これがあるべき機械学習の未来であると筆者は考える．</p>
</section>
<section id="つの世界樹" class="level3" data-number="1.3">
<h3 data-number="1.3" class="anchored" data-anchor-id="つの世界樹"><span class="header-section-number">1.3</span> ２つの世界樹</h3>
<p>今こそ，この２つの手法は根底では繋がっていることをよく周知し，この２つの視座を往来しながら適材適所に使うことが大事だと筆者は考える．</p>
<p>しかしそのためには，ベイズ機械学習の発展が遅れている現状を鑑みて，ベイズの手法のより一層の発展と理解の深化が必要である．<a href="#fn8" class="footnote-ref" id="fnref8" role="doc-noteref"><sup>8</sup></a></p>
<p>本章「ベイズ機械学習のすすめ」は，ベイズの手法の特に肝心と思われる３つの側面を指摘して終わる．以下３章を通じて，</p>
<ol type="1">
<li><p>第 <a href="#sec-uncertainty-quantification" class="quarto-xref">2</a> 節 <u><strong>ベイズは不確実性を定量化する</strong></u></p>
<p>Bayes の方が不確実性の定量化が得意であるため，そのような応用先では頻度論的な手法よりも，Bayes バージョンの手法を用いることが出来ると便利である．</p></li>
<li><p>第 <a href="#sec-distributional-representation" class="quarto-xref">3</a> 節 <u><strong>ベイズは分布という共通言語を与える</strong></u></p>
<p>Bayes による統一的な扱いが理論的に有用である場面が増えている．その際に，Bayes による理論解析と最適化による実際の推論という適材適所の協業が未来の方向であるかも知れない．</p></li>
<li><p>第 <a href="#sec-inductive-bias" class="quarto-xref">4</a> 節 <u><strong>ベイズは理解を促進する</strong></u></p>
<p>ベイズの手法が敬遠されていた理由も，換言すれば，「事前分布」という得体の知れないものを通じて，理論的深淵と直結するためである．ベイズ手法の研究が理論的な解明を要請する．だからこそ，数学者の魂を持った者がこの途を通ることは人類に大きく資すると筆者は考える．</p></li>
</ol>
</section>
</section>
<section id="sec-uncertainty-quantification" class="level2" data-number="2">
<h2 data-number="2" class="anchored" data-anchor-id="sec-uncertainty-quantification"><span class="header-section-number">2</span> ベイズは不確実性を定量化する</h2>
<section id="sec-need-for-uncertainty-quantification" class="level3" data-number="2.1">
<h3 data-number="2.1" class="anchored" data-anchor-id="sec-need-for-uncertainty-quantification"><span class="header-section-number">2.1</span> 不確実性の定量化の必要性</h3>
<p>機械学習と統計学が単なる道具ではなく，人間のより大きなシステムの一環を単独で担う場面が増えてきた．例えば，</p>
<ul>
<li>金融・経営・政策決定などの分野で，意思決定に繋げるデータ解析をするとき</li>
<li>科学において，発見や仮説を検証するためのデータ解析をするとき</li>
<li>ロボットや自動車などの自動化をし，社会に実装するとき<a href="#fn9" class="footnote-ref" id="fnref9" role="doc-noteref"><sup>9</sup></a></li>
<li>医療診断や裁判などの場面で，専門家を補助するシステムを作るとき</li>
</ul>
<p>これらのいずれの例でも，<u><strong>システムの一部を担うにあたって，不確実性を定量化しておくことが欠かせない</strong></u>．その出力を用いるのが人間である場合も勿論，別の機械学習モデルである場合は尚更である．</p>
<p>つまり，人間社会で優秀であるだけでなくホウレンソウと信頼獲得も重要であるように，機械学習モデルも性能の高さと正確さだけでなく，いつその結果を信頼して良いのかを「どの程度」という指標と共に知らせてくれることが信頼関係の基本となるだろう．</p>
<p>実際，殆どの場面で，データから高い確証度で言えることと，そうではないことでは全く違う意味を持つ．それぞれの場面での例には，次のようなものがあるだろう：</p>
<ul>
<li>データから高い確証度で言えることと，意思決定者による采配が必要な部分を分離できない限り，意思決定プロセスの一部として組み込むことが難しく，結局機械学習手法が全く採用されないということもあり得る．</li>
<li>結果の再現可能性が科学の基本的な要請である以上，その結果の不確実性を実験結果に付記することは基本的な科学的態度である．後述（第 <a href="#sec-replication-crisis" class="quarto-xref">2.3.1</a> 節）するが，<span class="math inline">\(p\)</span>-値や信頼区間などの統計量は<u><strong>これに応えるものではない</strong></u>．</li>
<li>ロボットや自動車の自動化 AI システムは，いくつかのモデルを組み合わせて作ることになるだろう．個々が十分な性能を持っていても，小さな誤差が累積してシステムとしての性能を著しく低下させることがある．これを防ぐために，統一した方法での不確実性の取り扱いが必要である．</li>
<li>個々人の権利と法益が衝突する場面にも AI が利用されより良い生活が実現されるには，法的な解釈可能性が担保される必要があることが，実は大きな難関として我々を待っている．その第一歩は，不確実性の可視化になるだろう．<a href="#fn10" class="footnote-ref" id="fnref10" role="doc-noteref"><sup>10</sup></a></li>
</ul>
<p>以上の内容は，結果の <strong>解釈可能性</strong> でも全く同じことが言えるだろう．</p>
</section>
<section id="信頼のおける-ai-システム" class="level3" data-number="2.2">
<h3 data-number="2.2" class="anchored" data-anchor-id="信頼のおける-ai-システム"><span class="header-section-number">2.2</span> 信頼のおける AI システム</h3>
<p>上述の点をまとめると，機械学習手法と人間社会がよりよく共生していくには AI の <strong>信頼性</strong> (trustworthyness) が必要とされているのである．不確実性の定量化と解釈可能性は，AI が人間社会で信頼を獲得するにあたって根本的な要素になるだろう．</p>
<p>現状の手法の延長でこの信頼性の問題は扱えず，新たな手法が必要とされている．Bayesian approach や probabilistic approach と呼ばれている試みは，まさにこれに応えるものであり，近年急速に発展している．</p>
</section>
<section id="sec-Bayes-for-uncertainty-quatification" class="level3" data-number="2.3">
<h3 data-number="2.3" class="anchored" data-anchor-id="sec-Bayes-for-uncertainty-quatification"><span class="header-section-number">2.3</span> 不確実性を扱うには Bayes が必要である</h3>
<p>実装は頻度論的な手法の方が簡単で高速であることが多いが，不確実性の定量化には向かない．</p>
<p>このような場面では，頻度論的手法を頻度論的に改善する，という方向は筋が悪いと思われる．<strong>このようなときこそ，もう一つの世界樹であるベイズの方法を用いるべきである</strong>．</p>
<p>これを，科学における再現性の危機を例にとって確認したい．<a href="#fn11" class="footnote-ref" id="fnref11" role="doc-noteref"><sup>11</sup></a></p>
<section id="sec-replication-crisis" class="level4" data-number="2.3.1">
<h4 data-number="2.3.1" class="anchored" data-anchor-id="sec-replication-crisis"><span class="header-section-number">2.3.1</span> 再現性の危機</h4>
<p>多くの実験科学では不確実性の定量化が必要不可欠である <span class="citation" data-cites="Krzywinski-Altman2013">(<a href="#ref-Krzywinski-Altman2013" role="doc-biblioref">Krzywinski and Altman, 2013</a>)</span>．</p>
<blockquote class="blockquote">
<p>It is necessary and true that all of the things we say in science, all of the conclusions, are uncertain … <span class="citation" data-cites="Feynman1998">(<a href="#ref-Feynman1998" role="doc-biblioref">Feynman, 1998</a>)</span></p>
</blockquote>
<p><a href="https://ja.wikipedia.org/wiki/%E5%86%8D%E7%8F%BE%E6%80%A7%E3%81%AE%E5%8D%B1%E6%A9%9F"><strong>再現性の危機</strong></a> (replication crisis) とは，多くの実験において報告されている統計的有意性が，再現実験において得られないことが多いという問題を指し，2010年代の初めから多くの科学分野において問題として取り上げられてきた．<a href="#fn12" class="footnote-ref" id="fnref12" role="doc-noteref"><sup>12</sup></a></p>
<p>その理由は明白である．<strong>信頼区間は集合値の推定量であるため，「分散」が十分大きいならば，データセットを変えて何回も計算することでいずれは非自明なものを得ることが出来るのである</strong>．そのため，信頼区間や <span class="math inline">\(P\)</span>-値を報告するだけでは，結果の信頼性については何も保証されないのである．</p>
<p>その結果多くの科学分野では <strong>Bayes 統計学による不確実性の定量化に移行しつつある</strong> <span class="citation" data-cites="Herzog-Ostwald2013">(<a href="#ref-Herzog-Ostwald2013" role="doc-biblioref">Herzog and Ostwald, 2013</a>)</span>, <span class="citation" data-cites="Trafimow-Marks2015">(<a href="#ref-Trafimow-Marks2015" role="doc-biblioref">Trafimow and Marks, 2015</a>)</span>, <span class="citation" data-cites="Nuzzo2014">(<a href="#ref-Nuzzo2014" role="doc-biblioref">Nuzzo, 2014</a>)</span>．</p>
<p>信頼区間と信用区間の違いに注目して，その違いを解説する．</p>
</section>
<section id="信頼区間と信用区間" class="level4" data-number="2.3.2">
<h4 data-number="2.3.2" class="anchored" data-anchor-id="信頼区間と信用区間"><span class="header-section-number">2.3.2</span> 信頼区間と信用区間</h4>
<p>「95 % の信頼区間」と言ったとき，「95 % の確率で真の値がその範囲に含まれるような区間」だと思いがちであるが，これはどちらかというと信用区間の説明であり，<strong>信頼区間は計算するごとに値が変わってしまう確率変数である</strong> ことを見落としがちである．<a href="#fn13" class="footnote-ref" id="fnref13" role="doc-noteref"><sup>13</sup></a></p>
<p>つまり，信頼区間は頻度論的な概念であり，「真の値」がまず存在し，区間自体が変動し，95 % の確率で被覆するというのである．今回見ている信頼区間が，別のデータセットで計算した場合にどう変わるかについては全く未知である．</p>
<p>このことは，信頼区間は「真のパラメータの値」で条件づけて得るものであるが，信用区間はデータによって条件づけて得るものであるという点で違う，とまとめられる．この２つの混同は「何で条件づけているか？」を意識することで回避することができる．<a href="#fn14" class="footnote-ref" id="fnref14" role="doc-noteref"><sup>14</sup></a></p>
<p>誤解を恐れず言うならば，再現性の危機とは，信頼区間というサイコロの出目によって科学が踊らされていたということに他ならない <span class="citation" data-cites="Nuzzo2014">(<a href="#ref-Nuzzo2014" role="doc-biblioref">Nuzzo, 2014</a>)</span>．<a href="#fn15" class="footnote-ref" id="fnref15" role="doc-noteref"><sup>15</sup></a></p>
</section>
<section id="なぜベイズを用いれば良いのか" class="level4" data-number="2.3.3">
<h4 data-number="2.3.3" class="anchored" data-anchor-id="なぜベイズを用いれば良いのか"><span class="header-section-number">2.3.3</span> なぜベイズを用いれば良いのか？</h4>
<p>これは，信頼区間や <span class="math inline">\(P\)</span>-値などの頻度論的な手法は，しばしば尤度原理に違反するためである．<a href="#fn16" class="footnote-ref" id="fnref16" role="doc-noteref"><sup>16</sup></a></p>
<p>換言すれば，何らかのモデルと事前分布に関するベイズ手法と等価である，すなわち，Bayesianly justifiable <span class="citation" data-cites="Rubin1984">(<a href="#ref-Rubin1984" role="doc-biblioref">Rubin, 1984</a>)</span> とみなせない手法は，何らかの意味でデータを十分に反映できていない可能性が高くなる．</p>
<p>従って，ベイズの手法が原理的に最も適切である場面が多い．一方でその計算の困難さや，全てのステップをモデリング段階に組み込む点を回避するために，種々の頻度論的な実装は考え得て，頻度論的な手法はそのような運用においては健全であるとの指標にもなる．<a href="#fn17" class="footnote-ref" id="fnref17" role="doc-noteref"><sup>17</sup></a></p>
<p>Bayes により手法を理解し，頻度論的に手法を実装することが，あるべき姿勢であると思われる．</p>
<blockquote class="blockquote">
<p>The applied statistician should be Bayesian in principle and calibrated to the real world in practice. <span class="citation" data-cites="Rubin1984">(<a href="#ref-Rubin1984" role="doc-biblioref">Rubin, 1984</a>)</span></p>
</blockquote>
</section>
</section>
<section id="ベイズ深層学習という夢" class="level3" data-number="2.4">
<h3 data-number="2.4" class="anchored" data-anchor-id="ベイズ深層学習という夢"><span class="header-section-number">2.4</span> ベイズ深層学習という夢</h3>
<p>深層モデルはその性能の高さから，最も実世界応用が期待されるモデルであるが，パラメータが極めて多いため，特にベイズ化することが難しいと言われている．</p>
<p>例えば，ハルシネーション (hallucination) として，LLM が「事実に基づかない」情報を生成してしまうことが問題とされているが，これも不確実性の定量化の問題に他ならない．<a href="#fn18" class="footnote-ref" id="fnref18" role="doc-noteref"><sup>18</sup></a></p>
<p>その他の場面でも，不確実性の定量化には conformal prediction などの事後的な手法が試みられている．<a href="#fn19" class="footnote-ref" id="fnref19" role="doc-noteref"><sup>19</sup></a> これらはどのようなブラックボックスに対しても適用可能である一方で，対症療法というべきものであり，ベイズ流の解釈をすることで直接的に事後分布を求めるという根本的な解決にも，もっと注力されるべきである．</p>
<p>ベイズによる不確実性の定量化は，自然であるだけでなく，より有用な不確実性の定量化を与えるものだと予想している．<a href="#fn20" class="footnote-ref" id="fnref20" role="doc-noteref"><sup>20</sup></a></p>
<p>加えて，事前分布を変えることで，種々の帰納バイアスを加えるという「プロンプトエンジニアリング」ならぬ「プライヤーエンジニアリング」の理論が樹立できるかもしれない．すでに，公平性，同変性，スパース性，共変量シフトへの頑健性などを達成するための事前分布が考えられている．<a href="#fn21" class="footnote-ref" id="fnref21" role="doc-noteref"><sup>21</sup></a></p>
</section>
<section id="分野全体の動向" class="level3" data-number="2.5">
<h3 data-number="2.5" class="anchored" data-anchor-id="分野全体の動向"><span class="header-section-number">2.5</span> 分野全体の動向</h3>
<p>現状の機械学習モデルと実応用との乖離は，他の側面でも生じている．</p>
<p>まず，訓練データが実際の運用環境を十分に反映できていないということは極めて頻繁に起こるだろう．この現象を <strong>分布シフト</strong> といい，機械学種モデルの予測性能だけで無く，分布外汎化 (out-of-distribution generalization) 能力も重視するという潮流が生じている．</p>
<p>さらに，一度訓練したモデルを，分布シフト自体が移り変わっていく環境で，微調整のみによって繰り返し使い続けるという使用を想定した <strong>継続学習</strong> (continual learning) という考え方もある．<a href="#fn22" class="footnote-ref" id="fnref22" role="doc-noteref"><sup>22</sup></a></p>
<p>章を変えて別の角度から議論を続けよう．</p>
</section>
</section>
<section id="sec-distributional-representation" class="level2" data-number="3">
<h2 data-number="3" class="anchored" data-anchor-id="sec-distributional-representation"><span class="header-section-number">3</span> ベイズは分布という共通言語を与える</h2>
<section id="継続学習という発想" class="level3" data-number="3.1">
<h3 data-number="3.1" class="anchored" data-anchor-id="継続学習という発想"><span class="header-section-number">3.1</span> 継続学習という発想</h3>
<p>継続学習は，機械学習モデルをより動的で実際的な環境でも使えるようにするための新たな枠組みである．そこまで，教師あり学習モデルがすでに実用的な性能を獲得したということでもある．</p>
<p>つまり，単に「教師あり」「教師なし」の１タスクを解く営みは爛熟しつつあり，機械学習の理論と応用の最先端は，より深い森に分け入りつつあるのである．</p>
<p>ここにおいて，ベイズ流の接近が統一的な取り扱いを与えるという美点が，さらに重要でもはや必要不可欠な役割を果たすものと思われる．</p>
<section id="ベイズ推論が与える統一的枠組み" class="level4" data-number="3.1.1">
<h4 data-number="3.1.1" class="anchored" data-anchor-id="ベイズ推論が与える統一的枠組み"><span class="header-section-number">3.1.1</span> ベイズ推論が与える統一的枠組み</h4>
<p>ベイズ推論とは，<strong>事前分布</strong> というものを設定して，これをデータによって更新するという営みである（その更新規則は Bayes の公式が与える）．</p>
<p>事前分布をどう設定すれば良いか？の問題は，ベイズ推論の初期からの問題であった．極めて自由度が高いことが，逆にベイズ推論が実際のデータ解析の場面において敬遠される一因ともなっていた．</p>
</section>
<section id="ベイズと最適化との協業" class="level4" data-number="3.1.2">
<h4 data-number="3.1.2" class="anchored" data-anchor-id="ベイズと最適化との協業"><span class="header-section-number">3.1.2</span> ベイズと最適化との協業</h4>
<p>しかし，継続学習が当たり前になった社会において，全てのパラメータ値を事前分布と事後分布とみなし，全ての学習過程をベイズの公式という統一的な方法で更新すると捉えられることは，極めて大きな利点になり得る．</p>
<p>というのも，継続学習においては，学習を繰り返すうちに過去に学んだ内容を忘れ去ってしまうという <strong>壊滅的忘却</strong> (catastrophically forgetting) が最大の困難である．</p>
<p>理論的には，分布のベイズ更新の繰り返しとして見る方が極めて見通しが良い．一方で，事後分布の近似が十分でない場合，実際にベイズ更新を行うことは性能に悪影響を与える．</p>
<p>そこで，理論解析や設計をベイズの観点から行い，実際の推論は最適化ベースで行うという適材適所により，壊滅的忘却を緩和できる可能性がある <span class="citation" data-cites="Farquhar-Gal2019">(<a href="#ref-Farquhar-Gal2019" role="doc-biblioref">Farquhar and Gal, 2019</a>)</span>．</p>
</section>
</section>
<section id="例強化学習への分布によるアプローチ" class="level3" data-number="3.2">
<h3 data-number="3.2" class="anchored" data-anchor-id="例強化学習への分布によるアプローチ"><span class="header-section-number">3.2</span> 例：強化学習への分布によるアプローチ</h3>
<blockquote class="blockquote">
<p>we believe the value distribution has a central role to play in reinforcement learning. <span class="citation" data-cites="Bellemare+2017">(<a href="#ref-Bellemare+2017" role="doc-biblioref">Bellemare et al., 2017</a>)</span></p>
</blockquote>
</section>
</section>
<section id="sec-inductive-bias" class="level2" data-number="4">
<h2 data-number="4" class="anchored" data-anchor-id="sec-inductive-bias"><span class="header-section-number">4</span> ベイズは理解を促進する</h2>
<p>我々はもはや機械学習を通じて，自分たちが何をやっているのかわかっていない．この愚かさを AI に継がせてはならない．</p>
<section id="なぜベイズ法の発展が遅れたか" class="level3" data-number="4.1">
<h3 data-number="4.1" class="anchored" data-anchor-id="なぜベイズ法の発展が遅れたか"><span class="header-section-number">4.1</span> なぜベイズ法の発展が遅れたか？</h3>
<p>ベイズ法の採用は，自分たちが何をやっているかへの理解と解釈可能性を刺激するという側面がある．</p>
<p>その理由は簡単である．ベイズ推論は，モデルとその上の事前分布を定めれば，あとはベイズ更新規則をどう計算するかの問題となり，近似手法は様々あれど，<strong>もはや推論手法に選択の余地はない</strong>．</p>
<p>換言すれば，その分解析者がモデルと事前分布の特定を全てこなす必要があるのであり，<strong>解析者に確率モデリングへの理解を強要する</strong>ところがある．</p>
<p>しかしこれは「面倒なことは全てアルゴリズムにやってほしい」という精神とは対立するため，ベイズの美点であると同時に，ベイズの発展を阻害してきた遠因の一つでもあった．</p>
<p>これを指して「事前分布の選択に恣意性が入る」という通り文句がよく使われるが，<u>実際は，頻度論的手法における「どのような目的関数をどのように最適化すれば良いか？」という恣意性に変換されているのみであり，問題を先送りにして，「ベイズ法 対 頻度論的手法」という虚構の対立を作り上げているのみである</u>．</p>
<p>機械学習のポテンシャルが具現化したいまこそ，この困難に立ち向かう必要があるが，この問題は最適化や頻度論的な立場から見るより，ベイズの立場から見た方が，理論的な見通しが良いようである（第 <a href="#sec-Bayesian-rule" class="quarto-xref">4.4</a> 節）．</p>
</section>
<section id="帰納バイアスの明確化の必要性" class="level3" data-number="4.2">
<h3 data-number="4.2" class="anchored" data-anchor-id="帰納バイアスの明確化の必要性"><span class="header-section-number">4.2</span> 帰納バイアスの明確化の必要性</h3>
<p>機械学習の真の理解のためには，各モデルの帰納バイアスを明確化する必要がある．</p>
<section id="帰納バイアスとは何か" class="level4" data-number="4.2.1">
<h4 data-number="4.2.1" class="anchored" data-anchor-id="帰納バイアスとは何か"><span class="header-section-number">4.2.1</span> 帰納バイアスとは何か？</h4>
<p>現状の AI システムは大量のラベル付きデータが必要であり，多くの現実的に有用なタスクでこのような教師データが用意できるわけではない．</p>
<p>一方で，人間は遥かに少ないデータから効率的に学習することができる．</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="Images/model_sizes.png" class="img-fluid figure-img"></p>
<figcaption>Number of Training Tokens <a href="https://babylm.github.io/">BabyLM Challenge</a></figcaption>
</figure>
</div>
<p>その違いは，進化が我々生物に授けた <strong>帰納バイアス</strong> にあると考えられている．</p>
<p>我々には遺伝的に継がれている生まれ持った学習特性があり，より効率的に学習出来るのかも知れない．</p>
<p>事実，一度事前学習をした LLM は，極めて少ないデータにより新しいタスクを学習することができるがわかりつつある <span class="citation" data-cites="Zhou+2023">(<a href="#ref-Zhou+2023" role="doc-biblioref">Zhou et al., 2023</a>)</span>．<a href="https://162348.github.io/posts/2024/Kernels/Deep2.html#sec-foundation-model">LLM の事後調整に関する稿</a> も参照．</p>
</section>
<section id="事前分布に向き合わずにやり過ごしてきた" class="level4" data-number="4.2.2">
<h4 data-number="4.2.2" class="anchored" data-anchor-id="事前分布に向き合わずにやり過ごしてきた"><span class="header-section-number">4.2.2</span> 事前分布に向き合わずにやり過ごしてきた</h4>
<p>現状，多くの機械学習手法は確率的な方法を取っていない．これは事前分布を明示せずに（ひょっとしたら明後日の方向に向かって）行われる Bayes 学習手法であるとみなせる．</p>
<p>現状の機械学習の成功は，事前分布に関する知識なしに到達されたものであり，それ故の限界がある．例えば，現状のままではモデルにどのような帰納バイアスが組み込まれているか不明瞭である．<a href="#fn23" class="footnote-ref" id="fnref23" role="doc-noteref"><sup>23</sup></a></p>
</section>
<section id="帰納バイアスに対するベイズ的視点" class="level4" data-number="4.2.3">
<h4 data-number="4.2.3" class="anchored" data-anchor-id="帰納バイアスに対するベイズ的視点"><span class="header-section-number">4.2.3</span> 帰納バイアスに対するベイズ的視点</h4>
<p>データの空間 <span class="math inline">\(\mathcal{X}\)</span> 上の任意のモデル <span class="math inline">\(\mathcal{M}\)</span> の周辺尤度 <span class="math inline">\(p(x|\mathcal{M})\)</span> は，<a href="#fn24" class="footnote-ref" id="fnref24" role="doc-noteref"><sup>24</sup></a> ベイズ流には事後確率として捉えられ，全てのデータ <span class="math inline">\(x\in\mathcal{X}\)</span> 上に有限な測度を定める．<a href="#fn25" class="footnote-ref" id="fnref25" role="doc-noteref"><sup>25</sup></a></p>
<p>よって，<strong>全てのモデルは，あるデータを得意とするならば他のデータについては不得意であることを免れない</strong>．これは no free lunch 定理と呼ばれる定理の一群により推測されており，分類問題などの簡単なタスクを除いて完全な形式的表現はまだ持たない作業仮設である．</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="Images/mackay.png" class="img-fluid figure-img"></p>
<figcaption>A Probabilistic Perspective of Genelization <span class="citation" data-cites="Wilson-Izmailov2020">(<a href="#ref-Wilson-Izmailov2020" role="doc-biblioref">Wilson and Izmailov, 2020</a>)</span></figcaption>
</figure>
</div>
<p>例えば，<a href="https://162348.github.io/posts/2024/Kernels/Deep2.html#sec-fine-tuning">基盤モデル</a> とは，インターネット上のデータから最大限人間の言語というものに関する帰納バイアスを取り込んだ，パラメータ上の初期設定であると見れる．</p>
<p>これは，あるパラメータ空間上の理想的な事前分布からのサンプリングであるかも知れない．それ故，種々の下流タスクに対して，小さなモデル変更のみにより適応することが出来る．</p>
<p>大規模言語モデルの能力創発現象は，帰納バイアスを十分取り込むことにより自然に解かれるタスクであったのかもしれない．</p>
</section>
<section id="worst-case-analysis-からの脱皮" class="level4" data-number="4.2.4">
<h4 data-number="4.2.4" class="anchored" data-anchor-id="worst-case-analysis-からの脱皮"><span class="header-section-number">4.2.4</span> worst-case analysis からの脱皮</h4>
<p>帰納バイアスを明確にせず，やり過ごしてきたつけが，特に学習理論においても現れている．</p>
<p>現状の統計的学習理論は全て，worst-case analysis であるが，実用上は全くそうではない．「動くモデル」には暗黙の帰納バイアスが入っており，これに明るくなる必要があるのである．</p>
<p>2024 年に生きる我々は，worst-case analysis からの脱皮を迫られている．</p>
</section>
</section>
<section id="数学者の哲学" class="level3" data-number="4.3">
<h3 data-number="4.3" class="anchored" data-anchor-id="数学者の哲学"><span class="header-section-number">4.3</span> 数学者の哲学</h3>
<p>Bayes の見方は，機械学習モデルを底流する数理的枠組みになっている．仮に次の <a href="https://ja.wikipedia.org/wiki/%E3%82%BD%E3%83%BC%E3%83%B3%E3%83%80%E3%83%BC%E3%82%B9%E3%83%BB%E3%83%9E%E3%83%83%E3%82%AF%E3%83%AC%E3%83%BC%E3%83%B3">Mac Lane</a> の言葉が数学者のあるべき態度の１つであるとするならば，この意味での数学者には Bayes の立場から機械学習を研究することを特におすすめする．</p>
<blockquote class="blockquote">
<p>However, I persisted in the position that <strong>as mathematicians we must know whereof we speak</strong>, be it a homotopy group or an adjoint functor. <span class="citation" data-cites="MacLane1983">(<a href="#ref-MacLane1983" role="doc-biblioref">Mac&nbsp;Lane, 1983, p. 55</a>)</span></p>
</blockquote>
<p>数理統計学に始まり，数学者の統計や機械学習分野への参入は，推論手法の解析が想像されるかも知れない．</p>
<p>しかし，真の数学的理解は，手法の数学的な機械仕掛けを紐解くだけでなく，それぞれの手法がモデルとしてどのような仮定の下で成り立っているかを，モデリングの観点から理解することにもあると筆者には思われる．</p>
<p>現状，後者の視点が大変に不足しており，数理的な知識に支えられた大局観というものがない．<u><strong>個々の数学的な道具に捉われず，大局的な構造を捉える数理的枠組みが必要である</strong></u>．</p>
<p>これに応えるのがベイズの枠組みであると筆者は信じる．</p>
<p>推論とモデリングという双対的な営みは深い数理的な構造を持っていることが明らかになりつつある．この大局的構造の解明と理論構築には，ベイズの観点から光を照らしてくれるような，<strong>Mac Lane の意味での数学者的な魂</strong>が必要とされているのである．</p>
<section id="bayes-の数学" class="level4" data-number="4.3.1">
<h4 data-number="4.3.1" class="anchored" data-anchor-id="bayes-の数学"><span class="header-section-number">4.3.1</span> Bayes の数学</h4>
<p>Bayes 流の解釈では，どんなにモデルが複雑で巨大になろうとも，推論とは積分に他ならない．</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="Images/Bayes.svg" class="img-fluid figure-img"></p>
<figcaption>Bayes’ Theorem (density form)</figcaption>
</figure>
</div>
<p>全ての（尤度原理に則った）推論は，事後分布の関数としてなされる（べきである）．</p>
<p>実際の実装は，その近似として実行される（べきである）．</p>
<p>よって，実装とモデリングの段階を明確に分離する枠組みを提供している上に，極めて普遍的な枠組みである．</p>
<p>というのも，Bayes 流のモデリングは，<a href="https://162348.github.io/posts/2023/Probability/MarkovCategory.html">Markov 圏</a> 上の図式と見ることができ（第 <a href="#sec-graphical-model" class="quarto-xref">5.2</a> 節），普遍的である上に，数学的にも最も直接的で直感的な表現であると思われる．</p>
<p>圏として持つ代数的性質は，モデルの結合・分解が自由に出来るということに繋がり，<a href="https://ja.wikipedia.org/wiki/%E3%83%A2%E3%82%B8%E3%83%A5%E3%83%BC%E3%83%AB">モジュール性</a> が高いということになる．</p>
<blockquote class="blockquote">
<p>I basically know of two principles for treating complicated systems in simple ways: the ﬁrst is the <strong>principle of modularity</strong> and the second is the <strong>principle of abstraction</strong>. I am an apologist for computational probability in machine learning because I believe that <strong>probability theory implements these two principles in deep and intriguing ways</strong> — namely through factorization and through averaging. Exploiting these two mechanisms as fully as possible seems to me to be the way forward in machine learning. Michael I. Jordan excerpted from <span class="citation" data-cites="Frey1998">(<a href="#ref-Frey1998" role="doc-biblioref">Frey, 1998</a>)</span></p>
</blockquote>
<p>分布を明示的に用いた <a href="https://162348.github.io/static/Notations.html#sec-kernels"><strong>確率核</strong></a> を通じてのモデリングは，なぜだか数学的に極めて自然なアプローチを提供してくれるようである．</p>
</section>
<section id="ベイズの代数幾何解析" class="level4" data-number="4.3.2">
<h4 data-number="4.3.2" class="anchored" data-anchor-id="ベイズの代数幾何解析"><span class="header-section-number">4.3.2</span> ベイズの代数・幾何・解析</h4>
<p>上述したように，ベイズのモデリング法と学習規則は本質的に代数的なところがある．</p>
<p>加えて，分布を基本言語とするために，ベイズ推論においては空間 <span class="math inline">\(\mathcal{P}(\mathcal{X})\subset\mathcal{M}^1(\mathcal{X})\)</span> が極めて基本的な役割を果たす．</p>
<p>サンプリングは <span class="math inline">\(\mathcal{P}(\mathcal{X})\)</span> 上の幾何学に関係が深く，情報幾何学や最適輸送などの発展が見られている．</p>
<p>一方で最適化は <span class="math inline">\(\mathcal{P}(\mathcal{X})\)</span> 上の解析学に関係が深く，古くから機械学習分野では <span class="math inline">\(\mathcal{P}(\mathcal{X})\)</span> 上の様々な汎函数が <strong>ダイバージェンス</strong> の名前で考察されており，その勾配流として種々の最適化手法が理解できる．</p>
</section>
<section id="bayes-に繋げる数学" class="level4" data-number="4.3.3">
<h4 data-number="4.3.3" class="anchored" data-anchor-id="bayes-に繋げる数学"><span class="header-section-number">4.3.3</span> Bayes に繋げる数学</h4>
<p>通常の頻度論的手法は，うまくいくことが先であり，理論が後付けされる．そしてその理論もどこか ad-hoc というべきであり，worst-case で漸近論的である．</p>
<p>これらに Bayes 的な解釈を与えることで，暗黙のうちにどのような仮定を課しているモデリング手法に相等するのか明確にされる．特に，非漸近論的な知見を与えてくれる数少ないの道の一つである．</p>
</section>
</section>
<section id="sec-Bayesian-rule" class="level3" data-number="4.4">
<h3 data-number="4.4" class="anchored" data-anchor-id="sec-Bayesian-rule"><span class="header-section-number">4.4</span> ベイズ推論とみる美点</h3>
<p>ベイズ推論自体への理解だけでなく，種々の頻度論的手法を（特定の環境下での）ベイズ推論の近似として理解することは，新たなアルゴリズムの開発に有用であるという合意が形成されつつあるようである．<a href="#fn26" class="footnote-ref" id="fnref26" role="doc-noteref"><sup>26</sup></a></p>
<p>最適化に基づく手法の計算効率性は，正確なベイズ推論に勝る場面も多い．ここで注意すべきは，ベイズ推論の実行が肝要であり，その実装は最適化に依ろうと，積分近似に依ろうと大した違いではないのである．</p>
<p>「ベイズ推論は多くの最尤法に基づく手法よりも，自然な正則化がなされるために過学習の問題がない．」と説明されるが<u><strong>本来は逆である</strong></u>．多くの最適化に基づく手法は，目的関数の選択に恣意性があり，その選択を誤り続けているために過学習という問題が生じている，という方が，後世の教科書に載る表現なのではないかと筆者は考えている．</p>
<p>そこで，種々の既存手法のベイズ流の解釈を探究することは，より良い推論アルゴリズムの開発に資すると考えられている．</p>
<p>この方向の近年の発展をいくつか紹介したい．</p>
<section id="ベイズ学習規則" class="level4" data-number="4.4.1">
<h4 data-number="4.4.1" class="anchored" data-anchor-id="ベイズ学習規則"><span class="header-section-number">4.4.1</span> ベイズ学習規則</h4>
<p>現状の機械学習は，統計学，連続最適化，計算機科学の知識を総動員して開発された種々の推論手法によって支えられている．</p>
<p>その性能は驚異的なスピードで向上しているが，それぞれの手法がどのような仮定をモデリングの段階で課しているかが不明瞭であり，どの手法を使うべきかの統一的な枠組みは得られていない．</p>
<p>この現状の抜本的な改善が，それぞれの手法のベイズ流の解釈を探究することで得られると考えられる．</p>
<p>その枠組みの一つが <strong>ベイズ学習規則</strong> <span class="citation" data-cites="Khan-Rue2023">(<a href="#ref-Khan-Rue2023" role="doc-biblioref">Khan and Rue, 2023</a>)</span> である．</p>
<p><span class="citation" data-cites="Khan-Rue2023">(<a href="#ref-Khan-Rue2023" role="doc-biblioref">Khan and Rue, 2023, p. 4</a>)</span> では，ベイズ流の解釈を持つ種々の手法が他より優れている理由として，目的関数に現れるエントロピー項が <strong>自然勾配</strong> の概念を通じて自然な正則化を与えることが，ベイズ学習規則という新たな理論的枠組みの中で示されている．</p>
</section>
<section id="例強化学習" class="level4" data-number="4.4.2">
<h4 data-number="4.4.2" class="anchored" data-anchor-id="例強化学習"><span class="header-section-number">4.4.2</span> 例：強化学習</h4>
<p>強化学習でも，モデルベースのアプローチが取り入れられつつあり <span class="citation" data-cites="Deisenroth-Rasmussen2011">(<a href="#ref-Deisenroth-Rasmussen2011" role="doc-biblioref">Deisenroth and Rasmussen, 2011</a>)</span>，さらに学習と制御をベイズ推論と見ることが，アルゴリズムの設計において有用であることが提唱されつつある：</p>
<blockquote class="blockquote">
<p>Crucially, in the framework of PGMs, it is sufficient to write down the model and pose the question, and the objectives for learning and inference emerge automatically. <span class="citation" data-cites="Levine2018">(<a href="#ref-Levine2018" role="doc-biblioref">Levine, 2018</a>)</span></p>
</blockquote>
</section>
</section>
</section>
<section id="bayes-機械学習の例" class="level2" data-number="5">
<h2 data-number="5" class="anchored" data-anchor-id="bayes-機械学習の例"><span class="header-section-number">5</span> Bayes 機械学習の例</h2>
<div class="callout callout-style-default callout-important no-icon callout-titled" title="要約">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
要約
</div>
</div>
<div class="callout-body-container callout-body">
<p>深層学習モデルにより教師あり学習は十分に発展し，多くの訓練データが得られる場面では驚異的な性能を発揮するようになった．</p>
<p>この発展は，モデリングの仮定に捉われずに純粋にアルゴリズムの開発に集中することが出来るという頻度論的な枠組みの利点を有効活用する形で達成された．</p>
<p>しかし，殆どの実世界応用では，不確実性のモデリングが必要不可欠である．この点を後回しにして性能を追求することで得た栄華である．だからこそ，極めて高い性能を誇るモデルを，実世界応用の場面で有効活用する手段を我々はまだ知らないのである．</p>
<p>その鍵はベイズにある．安全性，信頼性，柔軟性……．これらの21世紀の社会の要請に応えるためには，ベイズ機械学習手法の発展と，既存の手法のベイズ流の理解とが追いつくことが，第一歩である．</p>
</div>
</div>
<p>既存の深層学習モデルは，「教師あり学習」という枠組みや，画像の分類タスクや自然言語処理のタスクなど，広く周知された問題設定とデータセットが存在する．</p>
<p>一方で，ベイズ機械学習における対応物はまだ十分に周知されていないようである．</p>
<p>ベイズ機械学習では「損失を最小化する」という枠組みの中でなるべく性能の良い推論手法を探す，というわかりやすい枠組みがある訳ではないようである．</p>
<p>そこで，本章ではベイズ機械学習の近年の発展を概観することを試みる．</p>
<section id="bayes-深層学習" class="level3" data-number="5.1">
<h3 data-number="5.1" class="anchored" data-anchor-id="bayes-深層学習"><span class="header-section-number">5.1</span> Bayes 深層学習</h3>
<p>ニューラルネットワークモデルは，隠れ素子数が無限大になる極限において，Gauss 過程モデルに漸近することが知られている <span class="citation" data-cites="Neal1996">(<a href="#ref-Neal1996" role="doc-biblioref">Neal, 1996</a>)</span>．Gauss 過程とはノンパラメトリックなベイズ機械学習手法の代表である．この対応を通じて，深層学習のベイズ流の解釈が進められている．</p>
<p>この稿の執筆後，本稿をまとめるかのようなアブストラクトを持ったポジションペーパー <span class="citation" data-cites="Papamarkou+2024">(<a href="#ref-Papamarkou+2024" role="doc-biblioref">Papamarkou et al., 2024</a>)</span> が公開された</p>
<blockquote class="blockquote">
<p>In the current landscape of deep learning research, there is a predominant emphasis on achieving high predictive accuracy in supervised tasks involving large image and language datasets. However, a broader perspective reveals a multitude of overlooked metrics, tasks, and data types, such as uncertainty, active and continual learning, and scientific data, that demand attention. Bayesian deep learning (BDL) constitutes a promising avenue, offering advantages across these diverse settings. <span class="citation" data-cites="Papamarkou+2024">(<a href="#ref-Papamarkou+2024" role="doc-biblioref">Papamarkou et al., 2024</a>)</span></p>
</blockquote>
<p>深層学習をベイズ化することで，上にあげた</p>
<ul>
<li>不確実性の自然な定量化</li>
<li>継続学習への柔軟な接続</li>
<li>科学的営みの促進</li>
</ul>
<p>などが目指せる．特に，現状の大規模な基盤モデルをベイズ化する悲願を真っ向から論じている．</p>
</section>
<section id="sec-graphical-model" class="level3" data-number="5.2">
<h3 data-number="5.2" class="anchored" data-anchor-id="sec-graphical-model"><span class="header-section-number">5.2</span> 確率的グラフィカルモデル</h3>
<p>歴史的に，（確率的）モデリングは，主に（確率的）グラフィカルモデルを通じて機械学習の分野に導入された．</p>
<p>そのため，20世紀に入ったばかりの頃は，Bayes 機械学習の唯一の例は確率的グラフィカルモデルなのであった．<a href="#fn27" class="footnote-ref" id="fnref27" role="doc-noteref"><sup>27</sup></a></p>
<p>だが，確率的グラフィカルモデルは，極めて普遍的で，従来の因果推論・階層モデル・欠測モデル・潜在変数モデル・構造方程式モデルなどの発展を包含する統一的な枠組みであることをより広く認識すべきである．</p>
<section id="ベイジアンネットワーク" class="level4" data-number="5.2.1">
<h4 data-number="5.2.1" class="anchored" data-anchor-id="ベイジアンネットワーク"><span class="header-section-number">5.2.1</span> ベイジアンネットワーク</h4>
<p>Bayesian Network は Markov 圏上の図式であり，方向関係のある変数間の関係をモデリングする最も直接的な方法である．</p>
</section>
<section id="構造的因果モデル" class="level4" data-number="5.2.2">
<h4 data-number="5.2.2" class="anchored" data-anchor-id="構造的因果モデル"><span class="header-section-number">5.2.2</span> 構造的因果モデル</h4>
</section>
<section id="階層モデル" class="level4" data-number="5.2.3">
<h4 data-number="5.2.3" class="anchored" data-anchor-id="階層モデル"><span class="header-section-number">5.2.3</span> 階層モデル</h4>
<p>階層モデルとは，ベイズの枠組みでは，観測変数・潜在変数の区別なく，モデルを自由に結合出来る点を利用したモデリング手法である．</p>
</section>
<section id="モデルの属人化" class="level4" data-number="5.2.4">
<h4 data-number="5.2.4" class="anchored" data-anchor-id="モデルの属人化"><span class="header-section-number">5.2.4</span> モデルの属人化</h4>
<p>大きなデータも，属人化医療や推薦システムなど多くの文脈では小さなデータの寄せ集めであり，そうでなくともその構造を正しく捉え，全ての不確実性を取り入れた柔軟なモデリングをすることで，さらに密接な形で社会に取り入れることができる．<a href="#fn28" class="footnote-ref" id="fnref28" role="doc-noteref"><sup>28</sup></a></p>
</section>
</section>
<section id="確率的プログラミング" class="level3" data-number="5.3">
<h3 data-number="5.3" class="anchored" data-anchor-id="確率的プログラミング"><span class="header-section-number">5.3</span> 確率的プログラミング</h3>
<section id="アルゴリズムのプログラミングからモデルのプログラミングへ" class="level4" data-number="5.3.1">
<h4 data-number="5.3.1" class="anchored" data-anchor-id="アルゴリズムのプログラミングからモデルのプログラミングへ"><span class="header-section-number">5.3.1</span> アルゴリズムのプログラミングから，モデルのプログラミングへ</h4>
<p>ベイズ流の解釈では，解析者の恣意的な選択はモデリングの段階に集中しており，モデルが決定すれば推論手法は自動的に従う．</p>
<p>このパラダイムでは，推論手法は背後に隠し，解析者はモデリングに集中するための新たなプログラミング言語があっても良いはずである．</p>
<p>このような言語を <strong>確率的プログラミング</strong> (Probabilistic Programming) 言語と呼ぶ．</p>
</section>
<section id="確率的プログラミングはグラフィカルモデルの拡張である" class="level4" data-number="5.3.2">
<h4 data-number="5.3.2" class="anchored" data-anchor-id="確率的プログラミングはグラフィカルモデルの拡張である"><span class="header-section-number">5.3.2</span> 確率的プログラミングはグラフィカルモデルの拡張である</h4>
<p>確率的グラフィカルモデルをどのようにプログラムに落とし込むかというと，確率核をシミュレーターとして実装するのである．</p>
<p>逆に，シミュレーションが可能な限りどのようなモデルも実装できるので，確率的グラフィカルモデルの真の拡張であると言える．<a href="#fn29" class="footnote-ref" id="fnref29" role="doc-noteref"><sup>29</sup></a></p>
</section>
<section id="simulation-based-inference" class="level4" data-number="5.3.3">
<h4 data-number="5.3.3" class="anchored" data-anchor-id="simulation-based-inference"><span class="header-section-number">5.3.3</span> Simulation-based Inference</h4>
<p>上述の通り，シミュレーターがあればモデルが定義でき，モデルがあれば推論ができる．さらに，棄却法，重点サンプリング法，MCMC，SMC などの Monte Carlo 法のレパートリーにより，殆どあらゆるシミュレーションと推論が統一的に実行できる．これが Bayes 推論の強みである．</p>
</section>
</section>
<section id="bayes-最適化" class="level3" data-number="5.4">
<h3 data-number="5.4" class="anchored" data-anchor-id="bayes-最適化"><span class="header-section-number">5.4</span> Bayes 最適化</h3>
<p>ベイズはシステムの一部として自然に組み込まれると論じたが（第 <a href="#sec-need-for-uncertainty-quantification" class="quarto-xref">2.1</a> 節），現状その最先端をいくのがベイズ最適化の分野である．</p>
<p>ベイズ最適化は最も簡単な形では，未知の関数 <span class="math inline">\(f:X\to\mathbb{R}\)</span> の最大値点を求める問題を，<a href="https://en.wikipedia.org/wiki/Sequential_analysis">逐次意思決定問題</a> として解く手法である．</p>
<p>ベイズ数値計算 <span class="citation" data-cites="Hagen1991">(<a href="#ref-Hagen1991" role="doc-biblioref">O’Hagan, 1991</a>)</span> の現代的な再解釈とも捉えられる．<a href="#fn30" class="footnote-ref" id="fnref30" role="doc-noteref"><sup>30</sup></a></p>
<p>この際，未知の関数 <span class="math inline">\(f\)</span> を Gauss 過程などでモデリングし，不確実性の高い点からサンプル <span class="math inline">\(f(x_1),f(x_2),\cdots\)</span> を取って最も効率の良い方法で最大化していくことを目指す．</p>
<p>ベイズ最適化は多腕バンディット問題と関係が深く，２つの問題は共に一方向のエージェント・環境相互作用しか仮定していないという形での強化学習への入り口である．</p>
</section>
<section id="確率的データ圧縮" class="level3" data-number="5.5">
<h3 data-number="5.5" class="anchored" data-anchor-id="確率的データ圧縮"><span class="header-section-number">5.5</span> 確率的データ圧縮</h3>
<p>殆どの（可逆）データ圧縮アルゴリズムは，シンボルの列に対する確率的モデリングと等価である．<a href="#fn31" class="footnote-ref" id="fnref31" role="doc-noteref"><sup>31</sup></a> そしてモデルの予測精度が良いほど，データの圧縮率は高い．</p>
<p>したがって，より良いベイズ（ノンパラメトリック）モデルの開発と，より幅広いデータに対するデータ圧縮技術の発展とは両輪である．<a href="#fn32" class="footnote-ref" id="fnref32" role="doc-noteref"><sup>32</sup></a></p>
</section>
<section id="モデルの自動発見" class="level3" data-number="5.6">
<h3 data-number="5.6" class="anchored" data-anchor-id="モデルの自動発見"><span class="header-section-number">5.6</span> モデルの自動発見</h3>
<p>機械学習の精神の一つに，データからの知識獲得をなるべく自動化したいというものがある．</p>
<p>ベイズの方から，統計解析自体を自動化する Automatic Statistician <span class="citation" data-cites="Lloyd+2014">(<a href="#ref-Lloyd+2014" role="doc-biblioref">Lloyd et al., 2014</a>)</span> という試みがある．これはデータを説明するモデルを自動発見し，結果を自然言語でまとめてくれる上に，モデルに含まれる不確実性に関しても報告してくれる．</p>



</section>
</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" role="list">
<div id="ref-Amershi+2019" class="csl-entry" role="listitem">
Amershi, S., Weld, D., Vorvoreanu, M., Fourney, A., Nushi, B., Collisson, P., … Horvitz, E. (2019). <a href="https://doi.org/10.1145/3290605.3300233">Guidelines for human-AI interaction</a>. In <em>Proceedings of the 2019 CHI conference on human factors in computing systems</em>, pages 1–13. New York, NY, USA: Association for Computing Machinery.
</div>
<div id="ref-Angrist-Pischke2010" class="csl-entry" role="listitem">
Angrist, J. D., and Pischke, J.-S. (2010). <a href="http://www.jstor.org/stable/25703496">The credibility revolution in empirical economics: How better research design is taking the con out of econometrics</a>. <em>The Journal of Economic Perspectives</em>, <em>24</em>(2), 3–30.
</div>
<div id="ref-Bensal+2019" class="csl-entry" role="listitem">
Bansal, G., Nushi, B., Kamar, E., Weld, D. S., Lasecki, W. S., and Horvitz, E. (2019). <a href="https://doi.org/10.1609/aaai.v33i01.33012429">Updates in human-AI teams: Understanding and addressing the performance/compatibility tradeoff</a>. <em>Proceedings of the AAAI Conference on Artificial Intelligence</em>, <em>33</em>(01), 2429–2437.
</div>
<div id="ref-Bellemare+2017" class="csl-entry" role="listitem">
Bellemare, M. G., Dabney, W., and Munos, R. (2017). <a href="https://proceedings.mlr.press/v70/bellemare17a.html">A distributional perspective on reinforcement learning</a>. In D. Precup and Y. W. Teh, editors, <em>Proceedings of the 34th international conference on machine learning</em>,Vol. 70, pages 449–458. PMLR.
</div>
<div id="ref-Broderick+2023" class="csl-entry" role="listitem">
Broderick, T., Gelman, A., Meager, R., Smith, A. L., and Zheng, T. (2023). <a href="https://doi.org/10.1126/sciadv.abn3999">Toward a taxonomy of trust for probabilistic machine learning</a>. <em>Science Advances</em>, <em>9</em>(7), eabn3999.
</div>
<div id="ref-Chen+2023" class="csl-entry" role="listitem">
Chen, J., Monica, J., Chao, W.-L., and Campbell, M. (2023). <a href="https://arxiv.org/abs/2305.20044">Probabilistic uncertainty quantification of prediction models with application to visual localization</a>.
</div>
<div id="ref-Deisenroth-Rasmussen2011" class="csl-entry" role="listitem">
Deisenroth, M. P., and Rasmussen, C. E. (2011). PILCO: A model-based and data-efficient approach to policy search. In <em>Proceedings of the 28th international conference on international conference on machine learning</em>, pages 465–472. Madison, WI, USA: Omnipress.
</div>
<div id="ref-Efron1986" class="csl-entry" role="listitem">
Efron, B. (1986). <a href="http://www.jstor.org/stable/2683105">Why isn’t everyone a bayesian?</a> <em>The American Statistician</em>, <em>40</em>(1), 1–5.
</div>
<div id="ref-Farquhar-Gal2019" class="csl-entry" role="listitem">
Farquhar, S., and Gal, Y. (2019). <a href="https://arxiv.org/abs/1902.06494">A unifying bayesian view of continual learning</a>.
</div>
<div id="ref-Feynman1998" class="csl-entry" role="listitem">
Feynman, R. P. (1998). <em><a href="">The meaning of it all: Thoughts of a citizen scientist</a></em>. Addison-Wesley.
</div>
<div id="ref-Frey1998" class="csl-entry" role="listitem">
Frey, B. J. (1998). <em><a href="https://mitpress.mit.edu/9780262062022/graphical-models-for-machine-learning-and-digital-communication/">Graphical models for machine learning and digital communication</a></em>. The MIT Press.
</div>
<div id="ref-Gal-Ghahramani2016" class="csl-entry" role="listitem">
Gal, Y., and Ghahramani, Z. (2016). <a href="https://openreview.net/forum?id=3QxqXoJEyfp7y9wltP11">Bayesian convolutional neural networks with bernoulli approximate variational inference</a>. In <em>International conference on learning representations</em>.
</div>
<div id="ref-Ghahramani2015" class="csl-entry" role="listitem">
Ghahramani, Z. (2015). <a href="https://www.nature.com/articles/nature14541">Probabilistic machine learning and artificial intelligence</a>. <em>Nature</em>, <em>521</em>, 452–459.
</div>
<div id="ref-Herzog-Ostwald2013" class="csl-entry" role="listitem">
Herzog, S., and Ostwald, D. (2013). <a href="https://doi.org/10.1038/494035b">Sometimes bayesian statistics are better</a>. <em>Nature</em>, <em>494</em>(7435), 35–35.
</div>
<div id="ref-Khan-Rue2023" class="csl-entry" role="listitem">
Khan, M. E., and Rue, H. (2023). <a href="http://jmlr.org/papers/v24/22-0291.html">The bayesian learning rule</a>. <em>Journal of Machine Learning Research</em>, <em>24</em>(281), 1–46.
</div>
<div id="ref-Krzywinski-Altman2013" class="csl-entry" role="listitem">
Krzywinski, M., and Altman, N. (2013). <a href="https://doi.org/10.1038/nmeth.2613">Importance of being uncertain</a>. <em>Nature Methods</em>, <em>10</em>(9), 809–810.
</div>
<div id="ref-Levine2018" class="csl-entry" role="listitem">
Levine, S. (2018). <a href="https://arxiv.org/abs/1805.00909">Reinforcement learning and control as probabilistic inference: Tutorial and review</a>.
</div>
<div id="ref-Lloyd+2014" class="csl-entry" role="listitem">
Lloyd, J. R., Duvenaud, D., Grosse, R., Tenenbaum, J. B., and Ghahramani, Z. (2014). Automatic construction and natural-language description of nonparametric regression models. In <em>Proceedings of the twenty-eighth AAAI conference on artificial intelligence</em>, pages 1242–1250. Québec City, Québec, Canada: AAAI Press.
</div>
<div id="ref-MacLane1983" class="csl-entry" role="listitem">
Mac&nbsp;Lane, S. (1983). <a href="https://doi.org/10.1007/BF03026510">The health of mathematics</a>. <em>The Mathematical Intelligencer</em>, <em>5</em>(4), 53–56.
</div>
<div id="ref-Mohri-Hashimoto2024" class="csl-entry" role="listitem">
Mohri, C., and Hashimoto, T. (2024). <a href="https://arxiv.org/abs/2402.10978">Language models with conformal factuality guarantees</a>.
</div>
<div id="ref-Murphy2022" class="csl-entry" role="listitem">
Murphy, K. P. (2022). <em><a href="https://probml.github.io/pml-book/book1.html">Probabilistic machine learning: An introduction</a></em>. MIT Press.
</div>
<div id="ref-Neal1996" class="csl-entry" role="listitem">
Neal, R. M. (1996). <em><a href="https://link.springer.com/book/10.1007/978-1-4612-0745-0">Bayesian learning for neural networks</a></em>,Vol. 118. Springer New York.
</div>
<div id="ref-Neal-Hinton1998" class="csl-entry" role="listitem">
Neal, R. M., and Hinton, G. E. (1998). <a href="https://link.springer.com/chapter/10.1007/978-94-011-5014-9_12">Learning in graphical models</a>. In M. I. Jordan, editor, pages 355–368. Springer Dordrecht.
</div>
<div id="ref-Novello+2024" class="csl-entry" role="listitem">
Novello, P., Dalmau, J., and Andeol, L. (2024). <a href="https://arxiv.org/abs/2403.11532">Out-of-distribution detection should use conformal prediction (and vice-versa?)</a>.
</div>
<div id="ref-Nuzzo2014" class="csl-entry" role="listitem">
Nuzzo, R. (2014). <a href="https://doi.org/10.1038/506150a">Scientific method: Statistical errors</a>. <em>Nature</em>, <em>506</em>(7487), 150–152.
</div>
<div id="ref-Hagen1991" class="csl-entry" role="listitem">
O’Hagan, A. (1991). <a href="https://doi.org/10.1016/0378-3758(91)90002-V">Bayes–hermite quadrature</a>. <em>Journal of Statistical Planning and Inference</em>, <em>29</em>(3), 245–260.
</div>
<div id="ref-Papamarkou+2024" class="csl-entry" role="listitem">
Papamarkou, T., Skoularidou, M., Palla, K., Aitchison, L., Arbel, J., Dunson, D., … Zhang, R. (2024). <a href="https://arxiv.org/abs/2402.00809">Position paper: Bayesian deep learning in the age of large-scale AI</a>.
</div>
<div id="ref-Rubin1984" class="csl-entry" role="listitem">
Rubin, D. B. (1984). <a href="https://doi.org/10.1214/aos/1176346785"><span class="nocase">Bayesianly Justifiable and Relevant Frequency Calculations for the Applied Statistician</span></a>. <em>The Annals of Statistics</em>, <em>12</em>(4), 1151–1172.
</div>
<div id="ref-Steinruecken+2015" class="csl-entry" role="listitem">
Steinruecken, C., Ghahramani, Z., and MacKay, D. (2015). <a href="https://doi.org/10.1109/DCC.2015.77">Improving PPM with dynamic parameter updates</a>. In <em>2015 data compression conference</em>, pages 193–202.
</div>
<div id="ref-Trafimow-Marks2015" class="csl-entry" role="listitem">
Trafimow, D., and Marks, M. (2015). <a href="https://doi.org/10.1080/01973533.2015.1012991">Editorial</a>. <em>Basic and Applied Social Psychology</em>, <em>37</em>(1), 1–2.
</div>
<div id="ref-Wamg+2024" class="csl-entry" role="listitem">
Wang, L., Zhang, X., Su, H., and Zhu, J. (2024). <a href="https://doi.org/10.1109/TPAMI.2024.3367329">A comprehensive survey of continual learning: Theory, method and application</a>. <em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em>, 1–20.
</div>
<div id="ref-Wilson-Izmailov2020" class="csl-entry" role="listitem">
Wilson, A. G., and Izmailov, P. (2020). <a href="https://proceedings.neurips.cc/paper/2020/hash/322f62469c5e3c7dc3e58f5a4d1ea399-Abstract.html">Bayesian deep learning and a probabilistic perspective of generalization</a>. In <em>Proceedings of the 34th international conference on neural information processing systems</em>. Red Hook, NY, USA: Curran Associates Inc.
</div>
<div id="ref-Zhou+2023" class="csl-entry" role="listitem">
Zhou, C., Liu, P., Xu, P., Iyer, S., Sun, J., Mao, Y., … Levy, O. (2023). <a href="https://openreview.net/forum?id=KBMOKmX2he"><span>LIMA</span>: Less is more for alignment</a>. In <em>Thirty-seventh conference on neural information processing systems</em>.
</div>
<div id="ref-平石-中村2022" class="csl-entry" role="listitem">
平石界, and 中村大輝. (2022). <a href="https://www.jstage.jst.go.jp/article/jpssj/54/2/54_27/_article/-char/ja">心理学における再現性危機の10年―危機は克服されたのか，克服され得るのか―</a>. <em>科学哲学</em>, <em>54</em>(2), 27–50.
</div>
</div></section><section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>これは <a href="https://www.microsoft.com/en-us/research/project/guidelines-for-human-ai-interaction/">Human-AI interaction におけるガイドライン</a> <span class="citation" data-cites="Amershi+2019">(<a href="#ref-Amershi+2019" role="doc-biblioref">Amershi et al., 2019</a>)</span>, <span class="citation" data-cites="Bensal+2019">(<a href="#ref-Bensal+2019" role="doc-biblioref">Bansal et al., 2019</a>)</span> でも明確にされている点である．この方向への試みの代表がベイズ機械学習，というわけではないが，筆者はベイズ機械学習の興隆は信頼のおける AI システムの構築にための極めて盤石な土台になるだろうと論じる．<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>本稿執筆後に，ほとんど同じ論調を，深層学習や基盤モデルを中心に，遥かに明瞭に述べた論文 <span class="citation" data-cites="Papamarkou+2024">(<a href="#ref-Papamarkou+2024" role="doc-biblioref">Papamarkou et al., 2024</a>)</span> を見つけたので，賢明な読者はぜひこちらを参考にしていただきたい．<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p><span class="citation" data-cites="Broderick+2023">(<a href="#ref-Broderick+2023" role="doc-biblioref">Broderick et al., 2023, p. 2</a>)</span> など．<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p>この違いが「過学習」という現象に見舞われるかの違いでもある．“Fortunately, Bayesian approaches are not prone to this kind of overfitting since they average over, rather than fit, the parameters” <span class="citation" data-cites="Ghahramani2015">(<a href="#ref-Ghahramani2015" role="doc-biblioref">Ghahramani, 2015, p. 454</a>)</span>．<a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5"><p>“for Bayesian researchers the main computational problem is integration, whereas for much of the rest of the community the focus is on optimization of model parameters.” <span class="citation" data-cites="Ghahramani2015">(<a href="#ref-Ghahramani2015" role="doc-biblioref">Ghahramani, 2015, p. 454</a>)</span>．このように，その用いる手法も鮮やかに対照的に見えるが，積分は変分近似を通じて最適化問題としても解けるし，Lengevin 法や HMC などの最適化手法は積分問題を解ける．<a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn6"><p><span class="citation" data-cites="Broderick+2023">(<a href="#ref-Broderick+2023" role="doc-biblioref">Broderick et al., 2023</a>)</span> が極めて説得的にこの点を指摘している．<a href="#fnref6" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn7"><p>合理的な信念の度合い (degree of belief) は確率の公理を満たす必要がある，という主張は <a href="https://en.wikipedia.org/wiki/Cox%27s_theorem">Cox の名前でも呼ばれる</a>．この点から，Bayes の定理は，帰納的推論の確率論的な拡張だとも捉えられる．“This justifies the use of subjective Bayesian probabilistic representations in artificial intelligence.” “Probabilistic modelling also has some conceptual advantages over alternatives because it is a normative theory for learning in artificially intelligent systems.” <span class="citation" data-cites="Ghahramani2015">(<a href="#ref-Ghahramani2015" role="doc-biblioref">Ghahramani, 2015, p. 453</a>)</span>．<a href="#fnref7" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn8"><p>現状，日本にてベイズ機械学習を専業として研究を進めている人は <a href="https://emtiyaz.github.io/">Emtiyaz Khan</a> に限ると思われる．<span class="citation" data-cites="Ghahramani2015">(<a href="#ref-Ghahramani2015" role="doc-biblioref">Ghahramani, 2015, p. 452</a>)</span> でも “Probabilistic approaches have only recently become a mainstream approach to artificial intellifence, robotics, and machine learning.” と述べられている．<a href="#fnref8" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn9"><p>“The uncertainty quantification of prediction models (e.g., neural networks) is crucial for their adoption in many robotics applications. This is arguably as important as making accurate predictions, especially for safety-critical applications such as self-driving cars.” <span class="citation" data-cites="Chen+2023">(<a href="#ref-Chen+2023" role="doc-biblioref">Chen et al., 2023</a>)</span>．<a href="#fnref9" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn10"><p>モデルの予測結果に不確実性の定量化が伴われていたならば，モデルを信用出来ない場面で意思決定者がこれを信用したため責任があるのか，使用者には非難可能性がないのか，モデル設計者に過失があったと言えるのかの議論に，足場を与えることが出来るだろう．<a href="#fnref10" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn11"><p><span class="citation" data-cites="Gal-Ghahramani2016">(<a href="#ref-Gal-Ghahramani2016" role="doc-biblioref">Gal and Ghahramani, 2016</a>)</span> も参照．<a href="#fnref11" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn12"><p>心理学においては「再現性問題が大きく注目される大きな契機となった「超能力論文」が出版されたのが 2011 年である」 <span class="citation" data-cites="平石-中村2022">(<a href="#ref-平石-中村2022" role="doc-biblioref">平石界 and 中村大輝, 2022</a>)</span> ようである．計量経済学における <strong>信頼性革命</strong> <span class="citation" data-cites="Angrist-Pischke2010">(<a href="#ref-Angrist-Pischke2010" role="doc-biblioref">Angrist and Pischke, 2010</a>)</span> は，再現性の危機の，もう一つの革新的な解決法である．<a href="#fnref12" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn13"><p>「それでは，信頼区間は不確実性の正しい定量化を与えないではないか！」ということになるが，その通りなのである．<span class="math inline">\(P\)</span>-値を計算する過程とは，帰無仮説で条件付けているだけであり，データの関数でもある．<span class="math inline">\(P\)</span>-値の確率変数としての分散が大きいほど，何回か同じ実験を繰り返せばすぐに小さな <span class="math inline">\(P\)</span>-値が得られることになる．これは <a href="https://162348.github.io/posts/2023/数理法務/法律家のための統計数理2.html#sec-Bayes-problem"><strong>基準確率の誤謬</strong></a> と似ている．<a href="#fnref13" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn14"><p>“Confidence intervals suffer from an inverse inference problem that is not very different from that suffered by the NHSTP. In the NHSTP, the problem is in traversing the distance from the probability of the finding, given the null hypothesis, to the probability of the null hypothesis, given the finding.” <span class="citation" data-cites="Trafimow-Marks2015">(<a href="#ref-Trafimow-Marks2015" role="doc-biblioref">Trafimow and Marks, 2015</a>)</span><a href="#fnref14" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn15"><p><span class="citation" data-cites="Nuzzo2014">(<a href="#ref-Nuzzo2014" role="doc-biblioref">Nuzzo, 2014</a>)</span> には，Fisher が最初に用いてから，Neyman-Pearson 理論がこれを排除したものの，コミュニティが <span class="math inline">\(P\)</span>-値を誤解して都合の良いように利用するようになるまでに至った歴史が説明されている．<a href="#fnref15" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn16"><p><span class="citation" data-cites="Murphy2022">(<a href="#ref-Murphy2022" role="doc-biblioref">Murphy, 2022, p. 201</a>)</span> の議論も参照．<a href="#fnref16" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn17"><p><span class="citation" data-cites="Efron1986">(<a href="#ref-Efron1986" role="doc-biblioref">Efron, 1986</a>)</span> も示唆深い．<a href="#fnref17" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn18"><p><span class="citation" data-cites="Mohri-Hashimoto2024">(<a href="#ref-Mohri-Hashimoto2024" role="doc-biblioref">Mohri and Hashimoto, 2024</a>)</span>, <span class="citation" data-cites="Papamarkou+2024">(<a href="#ref-Papamarkou+2024" role="doc-biblioref">Papamarkou et al., 2024, p. 3</a>)</span> 2.1節 なども指摘している．<a href="#fnref18" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn19"><p><span class="citation" data-cites="Novello+2024">(<a href="#ref-Novello+2024" role="doc-biblioref">Novello et al., 2024</a>)</span> では out-of-distribution detection, <span class="citation" data-cites="Mohri-Hashimoto2024">(<a href="#ref-Mohri-Hashimoto2024" role="doc-biblioref">Mohri and Hashimoto, 2024</a>)</span> は LLM の hallucination への応用．<a href="#fnref19" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn20"><p>「筆者は，conformal prediction などの post-hoc な手法は，便利かも知れないが，「信頼区間」や「<span class="math inline">\(P\)</span>-値」のような側面（第 <a href="#sec-Bayes-for-uncertainty-quatification" class="quarto-xref">2.3</a> 節）も併せ持つのではないかと危惧しながら見ている．」と当初は書いていたが，どうもそう簡単な話ではないようである．<span class="citation" data-cites="Papamarkou+2024">(<a href="#ref-Papamarkou+2024" role="doc-biblioref">Papamarkou et al., 2024</a>)</span> を読んで思った．<a href="#fnref20" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn21"><p><span class="citation" data-cites="Papamarkou+2024">(<a href="#ref-Papamarkou+2024" role="doc-biblioref">Papamarkou et al., 2024, p. 5</a>)</span> 3.4節．<a href="#fnref21" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn22"><p><span class="citation" data-cites="Wamg+2024">(<a href="#ref-Wamg+2024" role="doc-biblioref">Wang et al., 2024</a>)</span> が最新のサーベイであるようだ．<a href="#fnref22" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn23"><p>Philipp Hennig <a href="https://youtu.be/TTo2kjrAuTo?si=QD_pqMkdLOl52OsR&amp;t=3703"><em>Probabilistic ML - Lecture 1 - Introduction</em></a> “Statistical Learning Theory is about Bayesian Reasoning when you don’t say out aloud what the prior is.”<a href="#fnref23" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn24"><p>これを <a href="https://en.wikipedia.org/wiki/Marginal_likelihood"><strong>証拠</strong></a> (model evidence) ともいう．<a href="#fnref24" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn25"><p>事前分布として非有限な測度を用いた場合など，例外もある．<a href="#fnref25" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn26"><p>“most conventional optimization-based machine-learning approaches have probabilistic analogues that handle uncertainty in a more principled manner.” <span class="citation" data-cites="Ghahramani2015">(<a href="#ref-Ghahramani2015" role="doc-biblioref">Ghahramani, 2015, p. 458</a>)</span>．<a href="#fnref26" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn27"><p><span class="citation" data-cites="Neal-Hinton1998">(<a href="#ref-Neal-Hinton1998" role="doc-biblioref">Neal and Hinton, 1998</a>)</span> など．<a href="#fnref27" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn28"><p><span class="citation" data-cites="Ghahramani2015">(<a href="#ref-Ghahramani2015" role="doc-biblioref">Ghahramani, 2015, p. 458</a>)</span> はこれを <strong>モデルの属人化</strong> (personalization of models) と呼んでいる．<a href="#fnref28" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn29"><p><span class="citation" data-cites="Ghahramani2015">(<a href="#ref-Ghahramani2015" role="doc-biblioref">Ghahramani, 2015, p. 453</a>)</span> “probabilistic programming offers an elegant way of generalizing graphical models, allowing a much richer representation of models.”．<a href="#fnref29" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn30"><p>“More generally, Bayesian optimization is a special case of Bayesian numerical computation, which is re-emerging as a very active area of research, and includes topics such as solving ordinary differential equations and numerical integration.” <span class="citation" data-cites="Ghahramani2015">(<a href="#ref-Ghahramani2015" role="doc-biblioref">Ghahramani, 2015, p. 456</a>)</span>．<a href="#fnref30" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn31"><p>“All commonly used lossless data compression algorithms (for example, <code>gzip</code>) can be viewed as probabilistic models of sequences of symbols.” <span class="citation" data-cites="Ghahramani2015">(<a href="#ref-Ghahramani2015" role="doc-biblioref">Ghahramani, 2015, p. 456</a>)</span>．<a href="#fnref31" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn32"><p><span class="citation" data-cites="Steinruecken+2015">(<a href="#ref-Steinruecken+2015" role="doc-biblioref">Steinruecken et al., 2015</a>)</span> は記号列に対するノンパラメトリックモデルを改良することで，データ圧縮アルゴリズム <code>PPM</code> を改良した良い例である．<a href="#fnref32" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/puniupa\.github\.io\/");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      const annoteTargets = window.document.querySelectorAll('.code-annotation-anchor');
      for (let i=0; i<annoteTargets.length; i++) {
        const annoteTarget = annoteTargets[i];
        const targetCell = annoteTarget.getAttribute("data-target-cell");
        const targetAnnotation = annoteTarget.getAttribute("data-target-annotation");
        const contentFn = () => {
          const content = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          if (content) {
            const tipContent = content.cloneNode(true);
            tipContent.classList.add("code-annotation-tip-content");
            return tipContent.outerHTML;
          }
        }
        const config = {
          allowHTML: true,
          content: contentFn,
          onShow: (instance) => {
            selectCodeLines(instance.reference);
            instance.reference.classList.add('code-annotation-active');
            window.tippy.hideAll();
          },
          onHide: (instance) => {
            unselectCodeLines();
            instance.reference.classList.remove('code-annotation-active');
          },
          maxWidth: 300,
          delay: [50, 0],
          duration: [200, 0],
          offset: [5, 10],
          arrow: true,
          trigger: 'click',
          appendTo: function(el) {
            return el.parentElement.parentElement.parentElement;
          },
          interactive: true,
          interactiveBorder: 10,
          theme: 'quarto',
          placement: 'right',
          positionFixed: true,
          popperOptions: {
            modifiers: [
            {
              name: 'flip',
              options: {
                flipVariations: false, // true by default
                allowedAutoPlacements: ['right'],
                fallbackPlacements: ['right', 'top', 'top-start', 'top-end', 'bottom', 'bottom-start', 'bottom-end', 'left'],
              },
            },
            {
              name: 'preventOverflow',
              options: {
                mainAxis: false,
                altAxis: false
              }
            }
            ]        
          }      
        };
        window.tippy(annoteTarget, config); 
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<script src="https://giscus.app/client.js" data-repo="puniupa/puniupa.github.io" data-repo-id="R_kgDOMOsIVA" data-category="Announcements" data-category-id="DIC_kwDOMOsIVM4CgazP" data-mapping="pathname" data-reactions-enabled="1" data-emit-metadata="0" data-input-position="top" data-theme="light" data-lang="en" crossorigin="anonymous" data-loading="lazy" async="">
</script>
<input type="hidden" id="giscus-base-theme" value="light">
<input type="hidden" id="giscus-alt-theme" value="dark">
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      <ul class="footer-items list-unstyled">
    <li class="nav-item">
    <a class="nav-link" href="https://puniupa.github.io/">
<p>ぷにうぱ</p>
</a>
  </li>  
</ul>
    </div>   
    <div class="nav-footer-center">
      &nbsp;
    </div>
    <div class="nav-footer-right">
      <ul class="footer-items list-unstyled">
    <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/puniupa/puniupa.github.io/">
      <i class="bi bi-github" role="img">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="mailto:puniupa48@gmail.com">
      <i class="bi bi-envelope" role="img">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="../../../blog.xml">
      <i class="bi bi-rss" role="img">
</i> 
    </a>
  </li>  
</ul>
    </div>
  </div>
</footer>
<script>videojs(video_shortcode_videojs_video1);</script>




</body></html>